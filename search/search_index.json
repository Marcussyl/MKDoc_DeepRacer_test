{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#what-is-aws-deepracer","title":"What is AWS DeepRacer?","text":"<p>AWS DeepRacer is the fastest way to get rolling with reinforcement learning (RL), featuring a fully autonomous, reinforcement learning-powered 1/18th scale race car, a 3D racing simulator, and an international racing league. Developers can participate in the AWS DeepRacer League for the chance to win the AWS DeepRacer Championship Cup, deploy their models onto the AWS DeepRacer for a real-world autonomous experience, and train, analyse, and tune RL models in the online simulator.</p> <p> Click to Know More about AWS DeepRacer</p>"},{"location":"#deepracer-workflow","title":"DeepRacer Workflow","text":"<pre><code>graph LR\n  A[New Model] --&gt; B[Configuration] --&gt; C[Train] --&gt; D[Evaluate] --&gt; E[Deploy to car];</code></pre>"},{"location":"#youtube-videos","title":"YouTube Videos","text":"<ul> <li> <p>https://www.youtube.com/watch?v=VwpFS1guqTg&amp;pp=ygUJZGVlcHJhY2Vy</p> </li> <li> <p>https://www.youtube.com/watch?v=vCt-F2HscOU&amp;pp=ygUJZGVlcHJhY2Vy</p> </li> <li> <p>https://www.youtube.com/watch?v=DAx42E9_Cug&amp;pp=ygUJZGVlcHJhY2Vy</p> </li> </ul>"},{"location":"events/","title":"What's on?","text":""},{"location":"events/#briefing-session","title":"Briefing Session","text":"<p> Date and time:  18:00-19:20 4/8/2023</p> <p> Venue: Innovation Wing Where is Innovation Wing?</p> <p> Material: Presentation Slides</p> <p> Zoom Link:</p>"},{"location":"events/#virtual-race","title":"Virtual Race","text":"<p> Date and time: 14:00-17:00 14/8/2023</p> <p> Track:</p> <p> Time Trail / Object Avoidance: Time Trail</p> <p> Invitation Link:</p>"},{"location":"events/#physical-race","title":"Physical Race","text":"<p> Date and time: 13:00-20:00 18/8/2023</p> <p> Venue:</p> <p> Time trail / Object avoidance: Time trail</p>"},{"location":"gallery/","title":"Gallery","text":""},{"location":"reinforcement_learning_intro/","title":"Reinforcement Learning","text":""},{"location":"reinforcement_learning_intro/#what-is-reinforcement-learning","title":"What is Reinforcement Learning?","text":"<p>Reinforcement learning is a type of machine learning that involves training agents to make decisions based on trial and error. In reinforcement learning, an agent learns to take actions in an environment to maximize a reward signal, which is a scalar value that indicates how well the agent is performing its task.</p> <pre><code>sequenceDiagram\n    Agent-&gt;&gt;Environment:Action\n    Environment-&gt;&gt;Agent:Status &amp; Reward</code></pre>"},{"location":"reinforcement_learning_intro/#reinforcement-learning-in-deepracer","title":"Reinforcement Learning in DeepRacer","text":""},{"location":"rules_and_guidelines/","title":"Rules and Guidelines","text":""},{"location":"rules_and_guidelines/#rules","title":"Rules","text":"<ol> <li>You may only compete with your own Reinforcement Learning models</li> <li>The race will be conducted in the time trial format , with winners determined as the fastest time on the leaderboard at the end of the race.</li> <li>The end of a lap is defined as the instance when the front wheel of the vehicle touches the finish line, which is the same way the lap starts initially.</li> <li>A total of 5 model submissions per participant is allowed.</li> <li>A total of 10 runs per participant is allowed. The 3 fastest &amp; valid results would be used to determine your time on the competition leaderboard. If the participant is unable to provide 3 valid results, they would be disqualified.</li> <li>Each participant must submit at least 1 model to the host before the competition day. Failure to do so would reduce your model submission count and run count by 1.</li> <li>If the vehicle goes out of the track (4 wheels all touching on the green part of the track at the same time), the run will not be considered as valid. If there are any uncertainty, a re-run will be organized.</li> <li>The time calculation is based on the stopwatch clicks of 2 referees, in the case of differences smaller than 1 second, the lap time will take the shorter click between 2 referees, in the case of differences larger than 1 second, the lap time will be taken according to recorded camera footage (30 fps capture rate, counting the frame when the front wheel of the vehicle touches the line from the previous lap relative to the current counting lap, to when the front wheel of the vehicle touches the line the current counting lap).</li> <li>Model submission ends at 8pm. The remaining un-used runs by participants will be considered as invalid.</li> </ol> Warning <ul> <li>The sharing and the use of shared models are strictly prohibited and will result in disqualification.</li> <li>Sufficient Proof of original authorship of models must be provided if asked. Failure to provide such proof would result in participant disqualification. (See Guideline 2 for Proof of authorship)</li> </ul>"},{"location":"rules_and_guidelines/#guidelines","title":"Guidelines","text":"<ol> <li>The running speed of the vehicle will be set to 80% during the race (may vary due to competition circumstances).</li> <li>The race track on the day of competition will be completely custom, so do not over fit your model to a single type of racetrack or sections (e.g. only right turns) train your model to fit a wider range of situations.</li> <li>Proof of authorship should be a short screen capture video of the process of your submission, from downloading from the dashboard to submitting to the submission form, plus a screenshot of the details of your model. Please try your best to adhere to the rules, keep your sportsmanship, and avoid any methods of foul play.</li> </ol> Warning <p> Beware of your free credits, we do not provide training credits for the purpose of this competition, there are student account options that provide more free credits, but use it at your own discretion.</p>"},{"location":"tutorial/","title":"Basic Concepts","text":""},{"location":"tutorial/#reward-function","title":"Reward Function","text":""},{"location":"tutorial/#what-is-reward-function","title":"What is reward function?","text":"<p>The reward function is python code that decribe immediate feedback in the form of reward and penalty to move from a given position on the track to a new position.</p>"},{"location":"tutorial/#whats-the-purpose-of-reward-function","title":"What's the purpose of reward function?","text":"<p>The reward function ecourages the vehicle to make moves along the track quickly to reach its destination! </p>"},{"location":"tutorial/#example","title":"Example","text":"<p>PAC MAN </p> <p></p> <p>Goal: collect all the pallets + avoid ghosts</p> <p>Rewards:</p> <p> Positive for pallets collected</p> <p> Negative for each time unit in contact with a ghost</p>"},{"location":"tutorial/#three-basic-reward-functions","title":"Three basic reward functions","text":"Follow Centerline <pre><code>    def reward_function(params):\n'''\n    Example of rewarding the agent to follow center line\n    '''\n# Read input parameters\ntrack_width = params['track_width']\ndistance_from_center = params['distance_from_center']\n# Calculate 3 markers that are at varying distances away from the center line\nmarker_1 = 0.1 * track_width\nmarker_2 = 0.25 * track_width\nmarker_3 = 0.5 * track_width\n# Give higher reward if the car is closer to center line and vice versa\nif distance_from_center &lt;= marker_1:\nreward = 1.0\nelif distance_from_center &lt;= marker_2:\nreward = 0.5\nelif distance_from_center &lt;= marker_3:\nreward = 0.1\nelse:\nreward = 1e-3  # likely crashed/ close to off track\nreturn float(reward)\n</code></pre> Stay within Borders <pre><code>    def reward_function(params):\n'''\n    Example of rewarding the agent to stay inside the two borders of the track\n    '''\n# Read input parameters\nall_wheels_on_track = params['all_wheels_on_track']\ndistance_from_center = params['distance_from_center']\ntrack_width = params['track_width']\n# Give a very low reward by default\nreward = 1e-3\n# Give a high reward if no wheels go off the track and\n# the agent is somewhere in between the track borders\nif all_wheels_on_track and (0.5*track_width - distance_from_center) &gt;= 0.05:\nreward = 1.0\n# Always return a float value\nreturn float(reward)\n</code></pre> Prevent zig-zag <pre><code>    def reward_function(params):\n'''\n    Example of penalize steering, which helps mitigate zig-zag behaviors\n    '''\n# Read input parameters\ndistance_from_center = params['distance_from_center']\ntrack_width = params['track_width']\nabs_steering = abs(params['steering_angle']) # Only need the absolute steering angle\n# Calculate 3 marks that are farther and father away from the center line\nmarker_1 = 0.1 * track_width\nmarker_2 = 0.25 * track_width\nmarker_3 = 0.5 * track_width\n# Give higher reward if the car is closer to center line and vice versa\nif distance_from_center &lt;= marker_1:\nreward = 1.0\nelif distance_from_center &lt;= marker_2:\nreward = 0.5\nelif distance_from_center &lt;= marker_3:\nreward = 0.1\nelse:\nreward = 1e-3  # likely crashed/ close to off track\n# Steering penality threshold, change the number based on your action space setting\nABS_STEERING_THRESHOLD = 15 \n# Penalize reward if the car is steering too much\nif abs_steering &gt; ABS_STEERING_THRESHOLD:\nreward *= 0.8\nreturn float(reward)\n</code></pre>"},{"location":"tutorial/#define-your-own-reward-function-mario-kart","title":"Define your own reward function (Mario Kart )","text":""},{"location":"tutorial/#before-coding-your-own-function","title":"Before coding your own function","text":"<p>Find out:</p> <ol> <li> <p>What status information do we have? e.g. Position of the car (shown in minimap)</p> </li> <li> <p>What actions do we have? e.g. LEFT [ 1, 0, 0 ], FORWARD  [ 0, 1, 0 ], RIGHT [ 0, 0, 1 ]</p> </li> <li> <p>When is reward calculated? (Sparsity of Reward Function) e.g. When the kart passed through a segment of the track / a checkpoint</p> </li> </ol>"},{"location":"tutorial/#reward-table","title":"Reward table","text":"<p>A reward table is a table that lists the rewards or penalties associated with different state-action pairs in a given environment. It is often used in reinforcement learning, a type of machine learning where an agent learns to take actions in an environment in order to maximize a cumulative reward signal.</p> STATUS REWARDS <code>Normal</code> -2 <code>Collision</code> -20 <code>Reaching a Checkpoint</code> 100 <code>Finishing a Lap</code> 150"},{"location":"tutorial/#overview","title":"Overview","text":""},{"location":"tutorial/#model-evaluation","title":"Model Evaluation","text":""},{"location":"tutorial/#reward-graph","title":"Reward Graph","text":"<p>A reward graph is a graphical representation of the rewards or penalties associated with different actions that an agent can take in a given environment. The goal of an AI agent is typically to maximize the cumulative reward it receives over time by choosing actions that lead to higher rewards.</p> Expand to see the graph <p></p> <p> Average reward</p> <p> Average percentage completion (training)</p> <p> Average percentage completion (evaluating)</p>"},{"location":"tutorial/#the-model-is-not-improving","title":"The Model is not Improving","text":"<p> Solution: Tune hyperparmeters e.g. increase epochs</p> Expand to see the graph <p></p>"},{"location":"tutorial/#the-model-is-overfitting","title":"The Model is Overfitting","text":"<p>Overfitting refers to a situation where a machine learning model is trained too well on a particular dataset, to the point that it starts to memorize the training data instead of learning the underlying patterns and relationships in the data. When a model overfits, it performs well on the training data but poorly on new, unseen data.</p> <p> Solution:</p> <ul> <li>Simplify your reward function</li> <li>Train with more tracks</li> <li>Reduce training time</li> </ul> Expand to see the graph <p></p>"},{"location":"tutorial/#recommended-readings","title":"Recommended readings","text":"<p> Reward function</p> <p> Reinforcement Learning</p> <p> Machine Learning</p>"}]}